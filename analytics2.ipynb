{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading data...\n",
      "INFO:root:Data read: (1000, 10)\n",
      "INFO:root:Starting Data Preparation (duplicates, coercion, normalization, outliers)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Climate Data Analysis...\n",
      "Input file: climate_change_dataset.csv\n",
      "Output directory: outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data Preparation completed. Cleaned shape: (1000, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (1000, 19)\n",
      "\n",
      "Column dtypes and non-null counts:\n",
      " - Year: dtype=int64, non-null=1000\n",
      " - Country: dtype=object, non-null=1000\n",
      " - Avg Temperature (°C): dtype=float64, non-null=1000\n",
      " - CO2 Emissions (Tons/Capita): dtype=float64, non-null=1000\n",
      " - Sea Level Rise (mm): dtype=float64, non-null=1000\n",
      " - Rainfall (mm): dtype=int64, non-null=1000\n",
      " - Population: dtype=int64, non-null=1000\n",
      " - Renewable Energy (%): dtype=float64, non-null=1000\n",
      " - Extreme Weather Events: dtype=int64, non-null=1000\n",
      " - Forest Area (%): dtype=float64, non-null=1000\n",
      " - Year_is_outlier: dtype=bool, non-null=1000\n",
      " - Avg Temperature (°C)_is_outlier: dtype=bool, non-null=1000\n",
      " - CO2 Emissions (Tons/Capita)_is_outlier: dtype=bool, non-null=1000\n",
      " - Sea Level Rise (mm)_is_outlier: dtype=bool, non-null=1000\n",
      " - Rainfall (mm)_is_outlier: dtype=bool, non-null=1000\n",
      " - Population_is_outlier: dtype=bool, non-null=1000\n",
      " - Renewable Energy (%)_is_outlier: dtype=bool, non-null=1000\n",
      " - Extreme Weather Events_is_outlier: dtype=bool, non-null=1000\n",
      " - Forest Area (%)_is_outlier: dtype=bool, non-null=1000\n",
      "\n",
      "Top values for categorical/object columns (up to 5):\n",
      " - Country: {'Indonesia': 75, 'Russia': 74, 'USA': 73, 'South Africa': 73, 'India': 70}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/12w34hx543d1fch2n5fqy4sr0000gn/T/ipykernel_24812/545109293.py:379: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(agg.index, rotation=45, ha=\"right\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- QUICK AUTO-SUMMARY (POST-PREP) ----\n",
      "Rows: 1000, Columns: 19\n",
      "Top 5 columns with most missing values:\n",
      " - Year: 0.0 missing (0.0%)\n",
      " - Year_is_outlier: 0.0 missing (0.0%)\n",
      " - Extreme Weather Events_is_outlier: 0.0 missing (0.0%)\n",
      " - Renewable Energy (%)_is_outlier: 0.0 missing (0.0%)\n",
      " - Population_is_outlier: 0.0 missing (0.0%)\n",
      "\n",
      "Detected important columns (guesses):\n",
      " - year: Year\n",
      " - country: Country\n",
      " - avg_temp: Avg Temperature (°C)\n",
      " - co2: CO2 Emissions (Tons/Capita)\n",
      " - sea_level: Sea Level Rise (mm)\n",
      " - rainfall: Rainfall (mm)\n",
      " - population: Population\n",
      " - renewable: Renewable Energy (%)\n",
      " - extreme_events: Extreme Weather Events\n",
      " - forest_area: Forest Area (%)\n",
      "\n",
      "Numeric columns count: 9\n",
      "\n",
      "Top absolute correlations (not self):\n",
      " - Year vs Population: 0.072\n",
      " - Renewable Energy (%) vs Avg Temperature (°C): 0.065\n",
      " - Sea Level Rise (mm) vs Avg Temperature (°C): 0.059\n",
      " - Forest Area (%) vs Year: 0.041\n",
      " - CO2 Emissions (Tons/Capita) vs Year: 0.041\n",
      " - Sea Level Rise (mm) vs CO2 Emissions (Tons/Capita): 0.039\n",
      " - Sea Level Rise (mm) vs Year: 0.035\n",
      " - Avg Temperature (°C) vs Extreme Weather Events: 0.035\n",
      " - Forest Area (%) vs CO2 Emissions (Tons/Capita): 0.031\n",
      " - Forest Area (%) vs Sea Level Rise (mm): 0.029\n",
      "\n",
      "Analysis completed successfully!\n",
      "Check the 'outputs' directory for all outputs.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Usage:\n",
    "    python climate_data_understanding.py --input path/to/data.csv --output outputs\n",
    "Produces:\n",
    " - textual summary saved as outputs/summary.txt\n",
    " - CSV reports: outputs/missing_report.csv, outputs/basic_stats.csv\n",
    " - saved plots in outputs/ (PNG)\n",
    " - preparation artifacts in outputs/modification/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from difflib import get_close_matches\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 6),\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"font.size\": 11,\n",
    "    \"figure.dpi\": 120\n",
    "})\n",
    "\n",
    "NUMERIC_DTYPES = [\"float64\", \"int64\", \"float32\", \"int32\"]\n",
    "\n",
    "def try_read(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".csv\", \".txt\"]:\n",
    "        return pd.read_csv(path)\n",
    "    elif ext in [\".xls\", \".xlsx\"]:\n",
    "        return pd.read_excel(path)\n",
    "    else:\n",
    "        # try csv as fallback\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "def find_best_match(col_candidates, choices, cutoff=0.6):\n",
    "    lowercase_map = {c.lower(): c for c in choices}\n",
    "    for cand in col_candidates:\n",
    "        if cand.lower() in lowercase_map:\n",
    "            return lowercase_map[cand.lower()]\n",
    "    for cand in col_candidates:\n",
    "        matches = get_close_matches(cand.lower(), [c.lower() for c in choices], n=1, cutoff=cutoff)\n",
    "        if matches:\n",
    "            return lowercase_map[matches[0]]\n",
    "    return None\n",
    "\n",
    "def detect_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    col_map = {}\n",
    "\n",
    "    candidates = {\n",
    "        \"year\": [\"Year\", \"year\"],\n",
    "        \"country\": [\"Country\", \"country\", \"region\", \"Location\"],\n",
    "        \"avg_temp\": [\"Average Temperature (°C)\", \"Average Temperature\", \"Avg Temp\", \"Temperature\", \"AvgTemperature\", \"avg_temp\"],\n",
    "        \"co2\": [\"CO2 Emissions (Tons/Capita)\", \"CO2 Emissions\", \"CO2\", \"CO2 Emissions (tons per capita)\"],\n",
    "        \"sea_level\": [\"Sea Level Rise (mm)\", \"Sea Level\", \"SeaLevel\", \"Sea Level Rise\", \"Sea_Level\"],\n",
    "        \"rainfall\": [\"Rainfall (mm)\", \"Rainfall\", \"Precipitation\", \"Rainfall_mm\"],\n",
    "        \"population\": [\"Population\", \"Pop\", \"Population Total\", \"Population (Total)\"],\n",
    "        \"renewable\": [\"Renewable Energy (%)\", \"Renewable Energy\", \"Renewable\", \"Renewable_pct\", \"Renewable (%)\"],\n",
    "        \"extreme_events\": [\"Extreme Weather Events\", \"Extreme Events\", \"Extreme_Events\", \"Extreme Weather\"],\n",
    "        \"forest_area\": [\"Forest Area (%)\", \"Forest Area\", \"Forest\", \"Forest_Area\"]\n",
    "    }\n",
    "\n",
    "    for key, cand_list in candidates.items():\n",
    "        found = find_best_match(cand_list, cols, cutoff=0.55)\n",
    "        col_map[key] = found\n",
    "\n",
    "    if col_map[\"year\"] is None:\n",
    "        for c in cols:\n",
    "            if np.issubdtype(df[c].dtype, np.number):\n",
    "                vals = df[c].dropna().unique()\n",
    "                if len(vals) > 0:\n",
    "                    mn, mx = vals.min(), vals.max()\n",
    "                    try:\n",
    "                        if 1800 <= int(mn) <= 2100 and 1800 <= int(mx) <= 2100:\n",
    "                            col_map[\"year\"] = c\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "    return col_map\n",
    "\n",
    "def summarize_structure(df, out_dir):\n",
    "    \"\"\"Print and save high-level structure: dtypes, non-null counts, top values for categoricals.\"\"\"\n",
    "    info = []\n",
    "    info.append(f\"DataFrame shape: {df.shape}\")\n",
    "    info.append(\"\\nColumn dtypes and non-null counts:\")\n",
    "    dtype_counts = df.dtypes.astype(str).to_dict()\n",
    "    non_null = df.notnull().sum().to_dict()\n",
    "    for c in df.columns:\n",
    "        info.append(f\" - {c}: dtype={dtype_counts[c]}, non-null={non_null[c]}\")\n",
    "    info.append(\"\\nTop values for categorical/object columns (up to 5):\")\n",
    "    for c in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        top = df[c].value_counts(dropna=True).head(5).to_dict()\n",
    "        info.append(f\" - {c}: {top}\")\n",
    "\n",
    "    summary_txt = \"\\n\".join(info)\n",
    "    with open(os.path.join(out_dir, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary_txt)\n",
    "\n",
    "    print(summary_txt)\n",
    "\n",
    "def missing_values_report(df, out_dir):\n",
    "    \"\"\"Compute and save missing value report (counts & percent).\"\"\"\n",
    "    miss = df.isnull().sum()\n",
    "    miss_pct = (miss / len(df) * 100).round(2)\n",
    "    report = pd.DataFrame({\"missing_count\": miss, \"missing_pct\": miss_pct})\n",
    "    report = report.sort_values(\"missing_count\", ascending=False)\n",
    "    report.to_csv(os.path.join(out_dir, \"missing_report.csv\"))\n",
    "    return report\n",
    "\n",
    "def basic_numeric_stats(df, out_dir):\n",
    "    \"\"\"Save basic numeric statistics to CSV and return DataFrame.\"\"\"\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    stats = num_df.describe().T\n",
    "    stats[\"missing_count\"] = num_df.isnull().sum()\n",
    "    stats.to_csv(os.path.join(out_dir, \"basic_stats.csv\"))\n",
    "    return stats\n",
    "\n",
    "def remove_duplicates(df, out_dir):\n",
    "    \"\"\"Find and drop duplicate rows, save report.\"\"\"\n",
    "    dup_mask = df.duplicated(keep='first')\n",
    "    dup_count = dup_mask.sum()\n",
    "    if dup_count > 0:\n",
    "        duplicates = df[dup_mask].copy()\n",
    "        duplicates.to_csv(os.path.join(out_dir, \"duplicates_removed_sample.csv\"), index=False)\n",
    "    else:\n",
    "        pd.DataFrame().to_csv(os.path.join(out_dir, \"duplicates_removed_sample.csv\"), index=False)\n",
    "    df_no_dup = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    with open(os.path.join(out_dir, \"duplicates_report.txt\"), \"w\") as f:\n",
    "        f.write(f\"duplicates_found={dup_count}\\nrows_before={len(df)}\\nrows_after={len(df_no_dup)}\\n\")\n",
    "    return df_no_dup, dup_count\n",
    "\n",
    "def coerce_numeric_like_columns(df, out_dir):\n",
    "    \"\"\"Attempt to convert object columns that look numeric into numeric dtype (remove commas).\"\"\"\n",
    "    coerced = []\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        sample = df[c].dropna().astype(str).head(200).str.replace(\",\", \"\").str.strip()\n",
    "        if sample.shape[0] > 0 and sample.str.match(r\"^-?\\d+(\\.\\d+)?$\").sum() > max(3, int(0.6*len(sample))):\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c].astype(str).str.replace(\",\", \"\").str.strip(), errors='coerce')\n",
    "                coerced.append(c)\n",
    "            except Exception:\n",
    "                pass\n",
    "    with open(os.path.join(out_dir, \"coerced_columns.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(coerced))\n",
    "    return df, coerced\n",
    "\n",
    "def normalize_percent_column(df, col, out_dir):\n",
    "    if col is None or col not in df.columns:\n",
    "        return df, None\n",
    "    s = df[col].dropna()\n",
    "    if s.empty:\n",
    "        return df, None\n",
    "    maxv = s.max()\n",
    "    minv = s.min()\n",
    "    action = None\n",
    "    if maxv <= 1.01 and minv >= 0:\n",
    "        df[col] = df[col].astype(float) * 100.0\n",
    "        action = \"scaled_0-1_to_0-100\"\n",
    "    if df[col].dtype == object:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str).str.replace(\"%\", \"\").str.strip()\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            action = \"stripped_percent_signs\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    # final clamp (0-100)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[col].dropna().max() <= 1.01:\n",
    "        df[col] = df[col] * 100.0\n",
    "        action = (action or \"\") + \"|ensure_scaled\"\n",
    "    with open(os.path.join(out_dir, \"percent_normalization.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"column={col}\\naction={action}\\nmin={df[col].min()}\\nmax={df[col].max()}\\n\")\n",
    "    return df, action\n",
    "\n",
    "def clean_population_column(df, col, out_dir):\n",
    "    \"\"\"Remove commas and coerce population to numeric.\"\"\"\n",
    "    if col is None or col not in df.columns:\n",
    "        return df, None\n",
    "    try:\n",
    "        df[col] = df[col].astype(str).str.replace(\",\", \"\").str.strip()\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        action = \"cleaned_commas_and_coerced\"\n",
    "    except Exception:\n",
    "        action = None\n",
    "    with open(os.path.join(out_dir, \"population_cleaning.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"column={col}\\naction={action}\\nmissing_after={df[col].isnull().sum()}\\n\")\n",
    "    return df, action\n",
    "\n",
    "def detect_flag_outliers(df, numeric_cols, out_dir, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers via IQR per column; create boolean flags column+'_is_outlier'.\n",
    "    Save outlier summary CSV with counts + example rows.\n",
    "    \"\"\"\n",
    "    outlier_summary = []\n",
    "    outlier_examples = []\n",
    "    for c in numeric_cols:\n",
    "        s = df[c].dropna()\n",
    "        if s.shape[0] < 10:\n",
    "            outlier_summary.append({\"column\": c, \"n_outliers\": 0, \"pct_outliers\": 0.0})\n",
    "            continue\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - iqr_multiplier * iqr\n",
    "        upper = q3 + iqr_multiplier * iqr\n",
    "        mask = (df[c] < lower) | (df[c] > upper)\n",
    "        n_out = int(mask.sum())\n",
    "        pct_out = round(100.0 * n_out / len(df), 3)\n",
    "        df[c + \"_is_outlier\"] = mask\n",
    "        outlier_summary.append({\"column\": c, \"n_outliers\": n_out, \"pct_outliers\": pct_out, \"lower\": lower, \"upper\": upper})\n",
    "        # save up to 20 example rows flagged as outliers\n",
    "        if n_out > 0:\n",
    "            ex = df.loc[mask, :].head(20)\n",
    "            ex_sample_path = os.path.join(out_dir, f\"outliers_examples_{c}.csv\")\n",
    "            ex.to_csv(ex_sample_path, index=False)\n",
    "            outlier_examples.append(ex_sample_path)\n",
    "    out_df = pd.DataFrame(outlier_summary).sort_values(\"n_outliers\", ascending=False)\n",
    "    out_df.to_csv(os.path.join(out_dir, \"outlier_summary.csv\"), index=False)\n",
    "    return df, out_df\n",
    "\n",
    "def winsorize_columns(df, numeric_cols, out_dir, iqr_multiplier=1.5):\n",
    "    df_w = df.copy()\n",
    "    caps = []\n",
    "    for c in numeric_cols:\n",
    "        s = df[c].dropna()\n",
    "        if s.shape[0] < 5:\n",
    "            continue\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - iqr_multiplier * iqr\n",
    "        upper = q3 + iqr_multiplier * iqr\n",
    "        df_w[c] = df_w[c].clip(lower=lower, upper=upper)\n",
    "        caps.append({\"column\": c, \"lower\": lower, \"upper\": upper})\n",
    "    pd.DataFrame(caps).to_csv(os.path.join(out_dir, \"winsorize_caps.csv\"), index=False)\n",
    "    df_w.to_csv(os.path.join(out_dir, \"cleaned_winsorized_data.csv\"), index=False)\n",
    "    return df_w, caps\n",
    "\n",
    "def save_modification_reports(original_df, cleaned_df, modification_dir):\n",
    "    report = {\n",
    "        \"rows_before\": len(original_df),\n",
    "        \"rows_after\": len(cleaned_df),\n",
    "        \"columns_before\": original_df.shape[1],\n",
    "        \"columns_after\": cleaned_df.shape[1]\n",
    "    }\n",
    "    with open(os.path.join(modification_dir, \"modification_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for k, v in report.items():\n",
    "            f.write(f\"{k}={v}\\n\")\n",
    "    return report\n",
    "\n",
    "# Plotting functions (matplotlib)\n",
    "\n",
    "def plot_missing_bar(report, out_path):\n",
    "    \"\"\"Bar chart of missing counts (top 30 columns).\"\"\"\n",
    "    top = report.head(30).sort_values(\"missing_count\", ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(8, max(4, len(top)*0.25)))\n",
    "    ax.barh(top.index, top[\"missing_count\"])\n",
    "    ax.set_xlabel(\"Missing values (count)\")\n",
    "    ax.set_title(\"Missing Values per Column (top 30)\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_histograms(df, numeric_cols, out_dir, bins=30, prefix=\"\"):\n",
    "    \"\"\"Plot histograms for numeric columns in a grid. Saves multiple figures if many columns.\"\"\"\n",
    "    cols = numeric_cols\n",
    "    per_fig = 6\n",
    "    for i in range(0, len(cols), per_fig):\n",
    "        subset = cols[i:i+per_fig]\n",
    "        n = len(subset)\n",
    "        rows = int(np.ceil(n / 2))\n",
    "        fig, axes = plt.subplots(rows, 2, figsize=(12, rows*3))\n",
    "        axes = axes.flatten()\n",
    "        for ax, col in zip(axes, subset):\n",
    "            data = df[col].dropna()\n",
    "            if data.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\")\n",
    "                continue\n",
    "            ax.hist(data, bins=bins, edgecolor=\"k\", alpha=0.7)\n",
    "            ax.set_title(col)\n",
    "            ax.set_ylabel(\"Count\")\n",
    "        # remove unused axes\n",
    "        for j in range(len(subset), len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        fig.suptitle(f\"Distributions of numeric features {prefix}\")\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        fig_path = os.path.join(out_dir, f\"histograms_{prefix}_{i//per_fig + 1}.png\").replace(\"//\", \"/\")\n",
    "        fig.savefig(fig_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "def plot_boxplots(df, numeric_cols, out_dir, prefix=\"\"):\n",
    "    \"\"\"Boxplots for numeric columns.\"\"\"\n",
    "    if len(numeric_cols) == 0:\n",
    "        return\n",
    "    per_fig = 8\n",
    "    for i in range(0, len(numeric_cols), per_fig):\n",
    "        subset = numeric_cols[i:i+per_fig]\n",
    "        fig, axes = plt.subplots(len(subset), 1, figsize=(10, len(subset)*1.6))\n",
    "        if len(subset) == 1:\n",
    "            axes = [axes]\n",
    "        for ax, col in zip(axes, subset):\n",
    "            data = df[col].dropna()\n",
    "            ax.boxplot(data, vert=False, patch_artist=True)\n",
    "            ax.set_title(col)\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(out_dir, f\"boxplots_{prefix}_{i//per_fig + 1}.png\").replace(\"//\", \"/\")\n",
    "        fig.savefig(fig_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "def plot_correlation_heatmap(df, numeric_cols, out_path, annot_threshold=0.2):\n",
    "    \"\"\"Correlation heatmap using matplotlib (annotated).\"\"\"\n",
    "    num_df = df[numeric_cols].copy().dropna(how=\"all\")\n",
    "    if num_df.shape[1] < 2:\n",
    "        return\n",
    "    corr = num_df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr.columns)))\n",
    "    ax.set_yticks(range(len(corr.columns)))\n",
    "    ax.set_xticklabels(corr.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(corr.columns)\n",
    "    # annotate\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(len(corr.columns)):\n",
    "            val = corr.iloc[i, j]\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.title(\"Correlation matrix (numeric features)\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_time_trends(df, year_col, cols_to_plot, out_path):\n",
    "    if year_col is None or year_col not in df.columns:\n",
    "        return\n",
    "    df_yr = df[[year_col] + cols_to_plot].copy()\n",
    "    df_yr = df_yr.dropna(subset=[year_col])\n",
    "    try:\n",
    "        df_yr[year_col] = df_yr[year_col].astype(int)\n",
    "    except Exception:\n",
    "        pass\n",
    "    grouped = df_yr.groupby(year_col).mean(numeric_only=True)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for col in grouped.columns:\n",
    "        ax.plot(grouped.index, grouped[col], label=col)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Mean value (per-year)\")\n",
    "    ax.set_title(\"Yearly mean trends (global)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_top_countries_bar(df, country_col, value_col, year_col=None, top_n=10, out_path=None):\n",
    "    if country_col is None or value_col is None:\n",
    "        return\n",
    "    tmp = df[[country_col, value_col]].copy()\n",
    "    latest_year = None\n",
    "    if year_col and year_col in df.columns:\n",
    "        # pick latest year available\n",
    "        latest_year = df[year_col].dropna().max()\n",
    "        tmp = df[df[year_col] == latest_year][[country_col, value_col]].copy()\n",
    "    agg = tmp.groupby(country_col)[value_col].mean(numeric_only=True).sort_values(ascending=False).head(top_n)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(agg.index.astype(str), agg.values)\n",
    "    ax.set_xticklabels(agg.index, rotation=45, ha=\"right\")\n",
    "    ax.set_title(f\"Top {top_n} countries by {value_col}\" + (f\" (year={latest_year})\" if latest_year is not None else \"\"))\n",
    "    ax.set_ylabel(value_col)\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def scatter_pair(df, x, y, out_path):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    data = df[[x, y]].dropna()\n",
    "    if data.empty:\n",
    "        return\n",
    "    ax.scatter(data[x], data[y], alpha=0.6, s=20)\n",
    "    # linear fit\n",
    "    try:\n",
    "        m, b = np.polyfit(data[x].values, data[y].values, deg=1)\n",
    "        xs = np.linspace(data[x].min(), data[x].max(), 100)\n",
    "        ax.plot(xs, m*xs + b, linestyle=\"--\", linewidth=1)\n",
    "        ax.text(0.02, 0.95, f\"y={m:.3f}x+{b:.3f}\", transform=ax.transAxes, fontsize=9, va=\"top\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    ax.set_title(f\"{y} vs {x}\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def prepare_data(df, col_map, modification_dir):\n",
    "    os.makedirs(modification_dir, exist_ok=True)\n",
    "    df0 = df.copy()\n",
    "    # 1) Duplicates\n",
    "    df_nd, dup_count = remove_duplicates(df0, modification_dir)\n",
    "\n",
    "    # 2) Coerce numeric-like columns\n",
    "    df_nd, coerced = coerce_numeric_like_columns(df_nd, modification_dir)\n",
    "\n",
    "    # 3) Normalize percent columns (renewable)\n",
    "    renewable_col = col_map.get(\"renewable\")\n",
    "    df_nd, percent_action = normalize_percent_column(df_nd, renewable_col, modification_dir)\n",
    "\n",
    "    # 4) Clean population column\n",
    "    pop_col = col_map.get(\"population\")\n",
    "    df_nd, pop_action = clean_population_column(df_nd, pop_col, modification_dir)\n",
    "\n",
    "    # 5) Detect numeric columns after coercion\n",
    "    numeric_cols = list(df_nd.select_dtypes(include=[np.number]).columns)\n",
    "    # remove 'is_outlier' leftover if present\n",
    "    numeric_cols = [c for c in numeric_cols if not c.endswith(\"_is_outlier\")]\n",
    "\n",
    "    # 6) Outlier detection (flag columns)\n",
    "    df_flagged, outlier_summary = detect_flag_outliers(df_nd, numeric_cols, modification_dir)\n",
    "\n",
    "    # 7) Winsorize numeric columns to produce cleaned dataset\n",
    "    df_wins, caps = winsorize_columns(df_flagged, numeric_cols, modification_dir)\n",
    "\n",
    "    # 8) Save both raw-nodup and winsorized versions\n",
    "    df_nd.to_csv(os.path.join(modification_dir, \"raw_no_duplicates.csv\"), index=False)\n",
    "    df_wins.to_csv(os.path.join(modification_dir, \"cleaned_winsorized_data.csv\"), index=False)\n",
    "\n",
    "    # 9) Save before/after histograms for a few key numeric columns (if available)\n",
    "    sample_cols = numeric_cols[:8]  # up to 8\n",
    "    if sample_cols:\n",
    "        plot_histograms(df_nd, sample_cols, modification_dir, prefix=\"before_prep\")\n",
    "        plot_histograms(df_wins, sample_cols, modification_dir, prefix=\"after_prep\")\n",
    "        plot_boxplots(df_nd, sample_cols, modification_dir, prefix=\"before_prep\")\n",
    "        plot_boxplots(df_wins, sample_cols, modification_dir, prefix=\"after_prep\")\n",
    "\n",
    "    # 10) Save modification summary\n",
    "    save_modification_reports(df0, df_wins, modification_dir)\n",
    "\n",
    "    # Returns winsorized df for downstream EDA\n",
    "    return df_wins\n",
    "\n",
    "def run_analysis(input_path, output_dir, top_n_countries=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    modification_dir = os.path.join(output_dir, \"modification\")\n",
    "    os.makedirs(modification_dir, exist_ok=True)\n",
    "\n",
    "    logging.info(\"Reading data...\")\n",
    "    df = try_read(input_path)\n",
    "    logging.info(f\"Data read: {df.shape}\")\n",
    "\n",
    "    # Detect important columns early (so preparation can treat them)\n",
    "    col_map = detect_columns(df)\n",
    "    pd.Series(col_map).to_json(os.path.join(output_dir, \"detected_columns.json\"), orient=\"index\", force_ascii=False)\n",
    "    logging.info(\"Starting Data Preparation (duplicates, coercion, normalization, outliers)...\")\n",
    "    df_clean = prepare_data(df, col_map, modification_dir)\n",
    "    logging.info(f\"Data Preparation completed. Cleaned shape: {df_clean.shape}\")\n",
    "\n",
    "    summarize_structure(df_clean, output_dir)\n",
    "\n",
    "    # Missing value report (on cleaned data)\n",
    "    missing_report = missing_values_report(df_clean, output_dir)\n",
    "    plot_missing_bar(missing_report, os.path.join(output_dir, \"missing_bar.png\"))\n",
    "\n",
    "    # Basic numeric stats (cleaned)\n",
    "    basic_stats = basic_numeric_stats(df_clean, output_dir)\n",
    "\n",
    "    # Re-detect columns on cleaned data (to ensure types in later steps)\n",
    "    col_map = detect_columns(df_clean)\n",
    "    pd.Series(col_map).to_json(os.path.join(output_dir, \"detected_columns_post_prep.json\"), orient=\"index\", force_ascii=False)\n",
    "\n",
    "    # Identify numeric columns for visualization\n",
    "    numeric_cols = list(df_clean.select_dtypes(include=[np.number]).columns)\n",
    "    numeric_cols = sorted(list(set([c for c in numeric_cols if not c.endswith(\"_is_outlier\")])))\n",
    "    with open(os.path.join(output_dir, \"numeric_columns.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(numeric_cols))\n",
    "\n",
    "    # Plots: distributions and boxplots\n",
    "    plot_histograms(df_clean, numeric_cols, output_dir)\n",
    "    plot_boxplots(df_clean, numeric_cols, output_dir)\n",
    "\n",
    "    # Correlation heatmap\n",
    "    if len(numeric_cols) >= 2:\n",
    "        plot_correlation_heatmap(df_clean, numeric_cols, os.path.join(output_dir, \"correlation_heatmap.png\"))\n",
    "\n",
    "    # Yearly trends: choose up to 4 interesting columns (if present)\n",
    "    trend_candidates = []\n",
    "    for key in [\"avg_temp\", \"co2\", \"sea_level\", \"renewable\", \"extreme_events\", \"rainfall\"]:\n",
    "        if col_map.get(key):\n",
    "            trend_candidates.append(col_map[key])\n",
    "    # ensure unique and numeric\n",
    "    trend_candidates = [c for c in trend_candidates if c in df_clean.columns and np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "    if len(trend_candidates) >= 1 and col_map.get(\"year\"):\n",
    "        plot_time_trends(df_clean, col_map[\"year\"], trend_candidates, os.path.join(output_dir, \"yearly_trends.png\"))\n",
    "\n",
    "    # Top countries by CO2 if column exists\n",
    "    if col_map.get(\"country\") and col_map.get(\"co2\"):\n",
    "        plot_top_countries_bar(df_clean, col_map[\"country\"], col_map[\"co2\"], year_col=col_map.get(\"year\"), top_n=top_n_countries, out_path=os.path.join(output_dir, \"top_countries_co2.png\"))\n",
    "\n",
    "    # Some targeted scatter pairs (if columns exist)\n",
    "    scatter_pairs = [\n",
    "        (\"co2\", \"renewable\"),\n",
    "        (\"avg_temp\", \"sea_level\"),\n",
    "        (\"extreme_events\", \"avg_temp\"),\n",
    "        (\"co2\", \"forest_area\")\n",
    "    ]\n",
    "    for x_key, y_key in scatter_pairs:\n",
    "        xcol = col_map.get(x_key)\n",
    "        ycol = col_map.get(y_key)\n",
    "        if xcol and ycol and xcol in df_clean.columns and ycol in df_clean.columns:\n",
    "            # ensure numeric\n",
    "            if np.issubdtype(df_clean[xcol].dtype, np.number) and np.issubdtype(df_clean[ycol].dtype, np.number):\n",
    "                out_path = os.path.join(output_dir, f\"scatter_{x_key}_vs_{y_key}.png\")\n",
    "                scatter_pair(df_clean, xcol, ycol, out_path)\n",
    "\n",
    "    # Final\n",
    "    summary_lines = []\n",
    "    summary_lines.append(\"---- QUICK AUTO-SUMMARY (POST-PREP) ----\")\n",
    "    summary_lines.append(f\"Rows: {len(df_clean)}, Columns: {df_clean.shape[1]}\")\n",
    "    summary_lines.append(\"Top 5 columns with most missing values:\")\n",
    "    summary_lines += [f\" - {idx}: {row['missing_count']} missing ({row['missing_pct']}%)\" for idx, row in missing_report.head(5).iterrows()]\n",
    "    summary_lines.append(\"\\nDetected important columns (guesses):\")\n",
    "    for k, v in col_map.items():\n",
    "        summary_lines.append(f\" - {k}: {v}\")\n",
    "    summary_lines.append(\"\\nNumeric columns count: \" + str(len(numeric_cols)))\n",
    "\n",
    "    if len(numeric_cols) >= 2:\n",
    "        corr = df_clean[numeric_cols].corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).drop_duplicates()\n",
    "        strong = corr[corr < 1.0].head(10)\n",
    "        summary_lines.append(\"\\nTop absolute correlations (not self):\")\n",
    "        for (c1, c2), val in strong.items():\n",
    "            summary_lines.append(f\" - {c1} vs {c2}: {val:.3f}\")\n",
    "    with open(os.path.join(output_dir, \"auto_summary_post_prep.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(summary_lines))\n",
    "    print(\"\\n\".join(summary_lines))\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Climate dataset EDA - Data Understanding (with Data Preparation)\")\n",
    "    parser.add_argument(\"--input\", \"-i\", required=True, help=\"Path to input CSV / Excel file\")\n",
    "    parser.add_argument(\"--output\", \"-o\", default=\"outputs\", help=\"Output directory for summaries & plots\")\n",
    "    parser.add_argument(\"--top_n\", \"-n\", type=int, default=10, help=\"Top N countries to plot for country-level charts\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# For notebook usage (instead of command line arguments)\n",
    "# Modify these parameters for your analysis\n",
    "input_path = \"climate_change_dataset.csv\"  # CHANGE THIS TO YOUR DATA FILE PATH\n",
    "output_dir = \"outputs\"                # Output directory\n",
    "top_n_countries = 10                  # Number of top countries to display\n",
    "\n",
    "# Set up logging for notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Starting Climate Data Analysis...\")\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    run_analysis(input_path, output_dir, top_n_countries)\n",
    "    print(\"\\nAnalysis completed successfully!\")\n",
    "    print(f\"Check the '{output_dir}' directory for all outputs.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File '{input_path}' not found!\")\n",
    "    print(\"Please update the 'input_path' variable above to point to your data file.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during analysis: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
